{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 500543568_530454874_MixPrec\n",
    "\n",
    "Team members: Andrew Zhang (SID:500543568) Vincent Yunansan (SID:530454874)\n",
    "\n",
    "This file generates mixed precision output for Experiment 2. Please run this file top to bottom on your colab GPU environment. All files can be accessed through the github repository:\n",
    "https://github.com/vyun8699/COMP4329-Deep-Learning-Assignment-2-Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#EDA imports\n",
    "import logging\n",
    "\n",
    "#basic libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd\n",
    "\n",
    "#load and counts and scores \n",
    "import os\n",
    "import re\n",
    "from io import StringIO\n",
    "from collections import Counter\n",
    "import time\n",
    "from datetime import datetime\n",
    "import glob\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "#nltk for word counts in caption\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#images\n",
    "from IPython.display import display, Image as DisplayImage\n",
    "from PIL import Image\n",
    "\n",
    "#torch\n",
    "import torch\n",
    "from torch.utils.data import random_split, Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "#train test split\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import images and csv files from git to colab. \n",
    "\n",
    "!mkdir -p data\n",
    "!git clone https://github.com/vyun8699/COMP4329-Deep-Learning-Assignment-2-Submission.git\n",
    "!find COMP4329-Deep-Learning-Assignment-2-Submission/data -type f -exec cp {} data/ \\;\n",
    "!rm -rf COMP4329-Deep-Learning-Assignment-2-Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read train file\n",
    "FILENAME = 'data/train.csv'\n",
    "with open(FILENAME) as file:\n",
    "    lines = [re.sub(r'([^,])\"(\\s*[^\\n])', r'\\1/\"\\2', line) for line in file]\n",
    "    train_data = pd.read_csv(StringIO(''.join(lines)), escapechar=\"/\")\n",
    "\n",
    "#read test file\n",
    "FILENAME = 'data/test.csv'\n",
    "with open(FILENAME) as file:\n",
    "    lines = [re.sub(r'([^,])\"(\\s*[^\\n])', r'\\1/\"\\2', line) for line in file]\n",
    "    test_data = pd.read_csv(StringIO(''.join(lines)), escapechar=\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n",
      "2.3.0+cu121\n",
      "12.1\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode_multilabels(df, label_column):\n",
    "    \"\"\"\n",
    "    One-hot encode a column of labels separated by spaces in a DataFrame.\n",
    "\n",
    "    input:\n",
    "    df (pd.DataFrame): The DataFrame containing the data.\n",
    "    label_column (str): The name of the column containing the space-separated labels.\n",
    "\n",
    "    output:\n",
    "    pd.DataFrame: A DataFrame with the original columns plus the one-hot encoded labels.\n",
    "    \"\"\"\n",
    "    # Check if the label column exists in DataFrame\n",
    "    if label_column not in df.columns:\n",
    "        raise ValueError(f\"The specified column '{label_column}' does not exist in the DataFrame.\")\n",
    "\n",
    "    # Split the labels by space and apply one-hot encoding\n",
    "    # We create a Series of lists of labels, then explode it to have a single label per row\n",
    "    s = df[label_column].str.split().explode()\n",
    "\n",
    "    # Convert the exploded labels into integers for proper one-hot encoding\n",
    "    s = s.astype(int)\n",
    "\n",
    "    # Get dummies and then sum back to original DataFrame shape\n",
    "    dummies = pd.get_dummies(s, prefix='Label').groupby(level=0).sum()\n",
    "\n",
    "    # Concatenate the original DataFrame with the new one-hot encoded columns\n",
    "    df_encoded = pd.concat([df, dummies], axis=1)\n",
    "\n",
    "    # Print counts for each label\n",
    "    label_counts = dummies.sum()\n",
    "    return df_encoded\n",
    "\n",
    "class EarlyStopper:\n",
    "    '''\n",
    "    Early stopping class to stop training if the validation loss does not improve after a certain number of epochs.\n",
    "    '''\n",
    "    def __init__(self, patience=4, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "# for train and val data\n",
    "class ImageDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset object for train image data (since it has both image and label)\n",
    "    input:\n",
    "        dataframe (pd.DataFrame): DataFrame containing the image paths and one-hot encoded labels.\n",
    "        image_dir (str): Directory where images are stored.\n",
    "        transform (callable, optional): Optional transform to be applied on a sample.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataframe, image_dir, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = os.path.join(self.image_dir, self.dataframe.iloc[idx, 0])\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        label = self.dataframe.iloc[idx, 1:].values.astype('float32') \n",
    "        \n",
    "        return image, label \n",
    "\n",
    "\n",
    "#for test data\n",
    "class ImageDataset2(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset object for test image data (since it only has image and no label)\n",
    "    input:\n",
    "        dataframe (pd.DataFrame): DataFrame containing the image paths and one-hot encoded labels.\n",
    "        image_dir (str): Directory where images are stored.\n",
    "        transform (callable, optional): Optional transform to be applied on a sample.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataframe, image_dir, transform=None):\n",
    "        self.dataframe = dataframe\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.image_dir, self.dataframe.iloc[idx, 0])\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "    \n",
    "#logger\n",
    "\n",
    "class Logger:\n",
    "    '''\n",
    "    Generate log files for analysis. This object runs in the background without user input.\n",
    "    '''\n",
    "    def __init__(self, model_name, timestamp, logging_path='models/logs/'):\n",
    "        self.model_name = model_name\n",
    "        self.timestamp = timestamp\n",
    "        self.logging_path = logging_path\n",
    "        os.makedirs(self.logging_path, exist_ok=True)\n",
    "\n",
    "        # Create a custom logger\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        self.logger.handlers = []  # Clear existing handlers, otherwise we'll get multiple as has happened\n",
    "        self.logger.setLevel(logging.INFO)\n",
    "\n",
    "        # Create handlers\n",
    "        c_handler = logging.StreamHandler()\n",
    "        f_handler = logging.FileHandler(f'{self.logging_path}{self.model_name}.log')\n",
    "        c_handler.setLevel(logging.INFO)\n",
    "        f_handler.setLevel(logging.INFO)\n",
    "\n",
    "        # Create formatters and add it to handlers\n",
    "        c_format = logging.Formatter('%(message)s')\n",
    "        f_format = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "        c_handler.setFormatter(c_format)\n",
    "        f_handler.setFormatter(f_format)\n",
    "\n",
    "        # Add handlers to the logger\n",
    "        self.logger.addHandler(c_handler)\n",
    "        self.logger.addHandler(f_handler)\n",
    "\n",
    "    def get_logger(self):\n",
    "        return self.logger\n",
    "\n",
    "#load cuda to run best model\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Running on the GPU: cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Running on the CPU\")\n",
    "    \n",
    "print(f'device: {device}')\n",
    "print(torch.__version__)\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#one hot encode train_data and drop excess columns\n",
    "df_encoded = one_hot_encode_multilabels(train_data, 'Labels')\n",
    "df_encoded_train = df_encoded.drop(columns=['Labels','Caption'])\n",
    "\n",
    "#train test split to create training and validation dataset\n",
    "\n",
    "train_proportion = 0.8\n",
    "dataframe = df_encoded_train\n",
    "image_dir = 'data/'\n",
    "\n",
    "# Split the dataset\n",
    "train_dataset, val_dataset = train_test_split(dataframe, test_size = 1 - train_proportion, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement transformations on train, val, and test datasets\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.RandomCrop((224,224)),\n",
    "    transforms.ColorJitter(brightness = 0.5),\n",
    "    transforms.RandomRotation(degrees = 45),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomVerticalFlip(p=0.05),\n",
    "    transforms.RandomGrayscale(p=0.2),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "transform_val = transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.CenterCrop((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.CenterCrop((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# create train, val, and test datasets\n",
    "train_dataset = ImageDataset(train_dataset, image_dir, transform=transform_train)\n",
    "val_dataset = ImageDataset(val_dataset, image_dir, transform=transform_val)\n",
    "test_dataset = ImageDataset2(dataframe = test_data, image_dir = image_dir, transform=transform_test)\n",
    "\n",
    "# create train, val and test dataloaders, not generator is set to None\n",
    "batch_size = 32\n",
    "generator2 = torch.Generator()\n",
    "generator2.manual_seed(42)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, generator=None)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, generator=None)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define training protocol\n",
    "- generate_predictions_and_save: generate prediction (csv file) based on trained model and test images\n",
    "- train_val_run: run training loop for model up to max epoch, while implementing early stopping and dynamic learning rate. This function calls generate_predictions_and_save to create csv output\n",
    "- run_experiments: implement a series of training by calling train_val_run for different combinations of optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_predictions_and_save(model,test_data, output_name, output_path = 'models/output/'):\n",
    "    '''\n",
    "    Generate prediction (csv file) based on trained model and test images\n",
    "    \n",
    "    input: \n",
    "    model: pth, trained model\n",
    "    test_data: df, test data\n",
    "    output_name: string, name of output file\n",
    "    output_path: string, path to save output file\n",
    "\n",
    "    output:\n",
    "    csv file, submitable to Kaggle\n",
    "    '''\n",
    "    #we don't explicitly call model in this function\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    # Generate prediction\n",
    "    test_predictions = []\n",
    "    test_image_ids = test_data['ImageID'].tolist()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images in test_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            predictions = torch.sigmoid(outputs).data > 0.5\n",
    "            test_predictions.extend(predictions.cpu().numpy())\n",
    "\n",
    "    # Get label names from df_encoded_train\n",
    "    label_columns = [col for col in df_encoded_train.columns if col.startswith('Label_')]\n",
    "    label_names = [col.split('_')[1] for col in label_columns]  # Adjust based on how labels are named\n",
    "\n",
    "    # Convert predictions to indices\n",
    "    predicted_labels = [' '.join(label_names[j] for j in range(len(label_names)) if pred[j]) for pred in test_predictions]\n",
    "\n",
    "    # Combine ImageID and predicted Labels into a DataFrame\n",
    "    results_df = pd.DataFrame({\n",
    "        'ImageID': test_image_ids,\n",
    "        'Labels': predicted_labels\n",
    "    })\n",
    "\n",
    "    output_file = f'{output_path}{output_name}.csv'\n",
    "\n",
    "    # Save as csv\n",
    "    results_df.to_csv(output_file, index=False)\n",
    "\n",
    "def train_val_run(model, optimizer, output_name, test_data, epochs, validate_every = 2, lr_scheduler_toggle = True):\n",
    "    '''\n",
    "    Run training loop for model up to max epoch, while implementing early stopping and dynamic learning rate. \n",
    "    \n",
    "    input: \n",
    "    model: pth, pre-trained model\n",
    "    optimizer: optimizer to use\n",
    "    output_name: string, name of output file\n",
    "    test_data: df, test data\n",
    "    epochs: integer, max epoch\n",
    "    validate_every: integer, frequency of validation\n",
    "    lr_scheduler_toggle: boolean, whether to use dynamic learning rate\n",
    "\n",
    "    output:\n",
    "    log file to be saved at designated path\n",
    "    model pth to be saved at designated path\n",
    "    csv file to be saved at designated path\n",
    "    '''\n",
    "    #extract determined model name and pre-trained model, assign to MPS/CUDA\n",
    "    model.to(device)\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    #setup scheduler\n",
    "    if lr_scheduler_toggle:\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2)\n",
    "\n",
    "    #set criterion \n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "    #setup loggers\n",
    "    timestamp = datetime.now().strftime('%Y%m%d%H%M')\n",
    "    logger = Logger(output_name,timestamp).get_logger()\n",
    "        \n",
    "    #record runtimes for stats\n",
    "    run_time = []\n",
    "\n",
    "    #implement early stopper to prevent overfitting\n",
    "    early_stopper = EarlyStopper(patience=6, min_delta=0.001)\n",
    "\n",
    "    #training and validation loop\n",
    "\n",
    "    start_epoch = 0\n",
    "\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        current_lr = optimizer.param_groups[0]['lr'] \n",
    "        \n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            with autocast(dtype = torch.float16):\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        epoch_time = time.time() - start_time\n",
    "        run_time.append(epoch_time)\n",
    "        logger.info(f'Training   >>> Epoch: {epoch+1}, Current LR: {current_lr}, Loss: {(running_loss/len(train_loader)):.3f}, Time: {int(epoch_time // 60)}m{int(epoch_time % 60)}s')\n",
    "\n",
    "        # Validation\n",
    "        if (epoch + 1) % validate_every == 0 or (epoch + 1) == epochs:\n",
    "            model.eval()\n",
    "            validation_loss = 0.0\n",
    "            all_labels = []\n",
    "            all_predictions = []\n",
    "            start_time = time.time()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for images, labels in val_loader:\n",
    "                    images, labels = images.to(device), labels.to(device)\n",
    "                    outputs = model(images)\n",
    "                    val_loss = criterion(outputs, labels)\n",
    "                    validation_loss += val_loss.item()\n",
    "                    predictions = torch.sigmoid(outputs).data > 0.5  \n",
    "                    all_labels.append(labels.cpu().numpy())\n",
    "                    all_predictions.append(predictions.cpu().numpy())\n",
    "\n",
    "            #calculate validation loss and update scheduler\n",
    "            validation_loss /= len(val_loader)\n",
    "            val_time = time.time() - start_time\n",
    "            run_time.append(val_time)\n",
    "\n",
    "            #calculate f1 score\n",
    "            all_labels = np.vstack(all_labels)\n",
    "            all_predictions = np.vstack(all_predictions)\n",
    "            f1_scores = f1_score(all_labels, all_predictions, average='macro')\n",
    "\n",
    "            logger.info(f'Validation >>> Epoch: {epoch+1}, Loss: {(validation_loss):.3f}, current lr: {current_lr}, F1 Score: {f1_scores:.3f}, Time: {int(val_time // 60)}m{int(val_time % 60)}s')\n",
    "\n",
    "            #updat scheduler\n",
    "            if lr_scheduler_toggle:\n",
    "                scheduler.step(validation_loss)\n",
    "\n",
    "            #early stopper\n",
    "            if early_stopper.early_stop(validation_loss):\n",
    "                logger.info(f'Early stopping >>> triggered at epoch {epoch + 1}')\n",
    "                break\n",
    "\n",
    "    #total time\n",
    "    total_time = sum(run_time)\n",
    "    logger.info(f'Training concluded >>> Total Run Time: {int(total_time // 60)}m{int(total_time % 60)}s')\n",
    "\n",
    "    #save model and show size\n",
    "\n",
    "    # Create directory if it doesn't exist\n",
    "    os.makedirs('models/pth', exist_ok=True)\n",
    "    path = f'models/pth/{output_name}.pth'\n",
    "    torch.save(model.state_dict(), path)\n",
    "    file_size = os.path.getsize(path)\n",
    "    logger.info(f\"Model size on disk: {file_size / (1024 * 1024):.2f} MB\")\n",
    "\n",
    "    #generate prediction output\n",
    "    generate_predictions_and_save(model,test_data, output_name)\n",
    "\n",
    "def run_experiments(model_name, model, epochs, optimizers, validate_every = 2):\n",
    "    '''\n",
    "    Implement a series of training by calling train_val_run for different combinations of optimizers\n",
    "\n",
    "    input:\n",
    "    model_name: string, name of model\n",
    "    model: pth, pre-trained model\n",
    "    epochs: integer, max epoch\n",
    "    optimizers: list of tuples, each tuple contains optimizer name, starting learning rate, and whether to use lr_scheduler\n",
    "    validate_every: integer, frequency of validation\n",
    "    '''\n",
    "    \n",
    "    for optimizer in optimizers:\n",
    "        optimizer_name = optimizer[0]\n",
    "        optimizer_starting_lr = optimizer[1]\n",
    "        optimizer_lr_scheduler_toggle = optimizer[2]\n",
    "        print(f'=========================================================================')\n",
    "        print(f\"Model: {model_name}, optimizer: {optimizer_name}, starting_lr= {optimizer_starting_lr}, lr_scheduler: {optimizer_lr_scheduler_toggle}\")\n",
    "        \n",
    "        if optimizer_name == 'Adadelta':\n",
    "            optimizer = torch.optim.Adadelta(model.parameters(), lr=optimizer_starting_lr)\n",
    "        elif optimizer_name == 'Adam':\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=optimizer_starting_lr) \n",
    "        elif optimizer_name == 'SGD':\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=optimizer_starting_lr)\n",
    "\n",
    "        output_name = f'{model_name}_{optimizer_name}_{optimizer_starting_lr}'\n",
    "        model = train_val_run(model, optimizer, output_name, test_data, epochs, lr_scheduler_toggle = optimizer_lr_scheduler_toggle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mix Precision Training for RegNet with AdaDelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================+++++++=============================\n",
      "Model: regnet, optimizer: Adadelta, starting_lr= 0.01, lr_scheduler: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/HDD5/andrew/Applications/anaconda3/lib/python3.11/site-packages/torch/nn/modules/conv.py:456: UserWarning: Plan failed with a cudnnException: CUDNN_BACKEND_EXECUTION_PLAN_DESCRIPTOR: cudnnFinalize Descriptor Failed cudnn_status: CUDNN_STATUS_NOT_SUPPORTED (Triggered internally at ../aten/src/ATen/native/cudnn/Conv_v8.cpp:919.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n",
      "Training   >>> Epoch: 1, Current LR: 0.01, Loss: 0.250, Time: 3m19s\n",
      "Training   >>> Epoch: 2, Current LR: 0.01, Loss: 0.172, Time: 3m16s\n",
      "Validation >>> Epoch: 2, Loss: 0.168, current lr: 0.01, F1 Score: 0.062, Time: 1m9s\n",
      "Training   >>> Epoch: 3, Current LR: 0.01, Loss: 0.158, Time: 3m13s\n",
      "Training   >>> Epoch: 4, Current LR: 0.01, Loss: 0.146, Time: 3m14s\n",
      "Validation >>> Epoch: 4, Loss: 0.140, current lr: 0.01, F1 Score: 0.240, Time: 0m29s\n",
      "Training   >>> Epoch: 5, Current LR: 0.01, Loss: 0.138, Time: 3m12s\n",
      "Training   >>> Epoch: 6, Current LR: 0.01, Loss: 0.132, Time: 3m14s\n",
      "Validation >>> Epoch: 6, Loss: 0.124, current lr: 0.01, F1 Score: 0.388, Time: 0m28s\n",
      "Training   >>> Epoch: 7, Current LR: 0.01, Loss: 0.126, Time: 3m11s\n",
      "Training   >>> Epoch: 8, Current LR: 0.01, Loss: 0.123, Time: 3m13s\n",
      "Validation >>> Epoch: 8, Loss: 0.114, current lr: 0.01, F1 Score: 0.461, Time: 0m29s\n",
      "Training   >>> Epoch: 9, Current LR: 0.01, Loss: 0.120, Time: 3m11s\n",
      "Training   >>> Epoch: 10, Current LR: 0.01, Loss: 0.117, Time: 3m12s\n",
      "Validation >>> Epoch: 10, Loss: 0.108, current lr: 0.01, F1 Score: 0.505, Time: 0m29s\n",
      "Training   >>> Epoch: 11, Current LR: 0.01, Loss: 0.114, Time: 3m12s\n",
      "Training   >>> Epoch: 12, Current LR: 0.01, Loss: 0.113, Time: 3m12s\n",
      "Validation >>> Epoch: 12, Loss: 0.104, current lr: 0.01, F1 Score: 0.545, Time: 0m27s\n",
      "Training   >>> Epoch: 13, Current LR: 0.01, Loss: 0.111, Time: 3m12s\n",
      "Training   >>> Epoch: 14, Current LR: 0.01, Loss: 0.109, Time: 3m13s\n",
      "Validation >>> Epoch: 14, Loss: 0.101, current lr: 0.01, F1 Score: 0.569, Time: 0m29s\n",
      "Training   >>> Epoch: 15, Current LR: 0.01, Loss: 0.108, Time: 3m11s\n",
      "Training   >>> Epoch: 16, Current LR: 0.01, Loss: 0.107, Time: 3m10s\n",
      "Validation >>> Epoch: 16, Loss: 0.097, current lr: 0.01, F1 Score: 0.590, Time: 0m27s\n",
      "Training   >>> Epoch: 17, Current LR: 0.01, Loss: 0.105, Time: 3m12s\n",
      "Training   >>> Epoch: 18, Current LR: 0.01, Loss: 0.105, Time: 3m14s\n",
      "Validation >>> Epoch: 18, Loss: 0.095, current lr: 0.01, F1 Score: 0.606, Time: 0m29s\n",
      "Training   >>> Epoch: 19, Current LR: 0.01, Loss: 0.103, Time: 3m12s\n",
      "Training   >>> Epoch: 20, Current LR: 0.01, Loss: 0.102, Time: 3m12s\n",
      "Validation >>> Epoch: 20, Loss: 0.094, current lr: 0.01, F1 Score: 0.617, Time: 0m28s\n",
      "Training   >>> Epoch: 21, Current LR: 0.01, Loss: 0.102, Time: 3m11s\n",
      "Training   >>> Epoch: 22, Current LR: 0.01, Loss: 0.100, Time: 3m9s\n",
      "Validation >>> Epoch: 22, Loss: 0.093, current lr: 0.01, F1 Score: 0.629, Time: 0m28s\n",
      "Training   >>> Epoch: 23, Current LR: 0.01, Loss: 0.100, Time: 3m12s\n",
      "Training   >>> Epoch: 24, Current LR: 0.01, Loss: 0.099, Time: 3m12s\n",
      "Validation >>> Epoch: 24, Loss: 0.091, current lr: 0.01, F1 Score: 0.644, Time: 0m29s\n",
      "Training   >>> Epoch: 25, Current LR: 0.01, Loss: 0.099, Time: 3m11s\n",
      "Training   >>> Epoch: 26, Current LR: 0.01, Loss: 0.097, Time: 3m13s\n",
      "Validation >>> Epoch: 26, Loss: 0.090, current lr: 0.01, F1 Score: 0.649, Time: 0m28s\n",
      "Training   >>> Epoch: 27, Current LR: 0.01, Loss: 0.097, Time: 3m12s\n",
      "Training   >>> Epoch: 28, Current LR: 0.01, Loss: 0.096, Time: 3m13s\n",
      "Validation >>> Epoch: 28, Loss: 0.089, current lr: 0.01, F1 Score: 0.651, Time: 0m29s\n",
      "Training   >>> Epoch: 29, Current LR: 0.01, Loss: 0.095, Time: 3m12s\n",
      "Training   >>> Epoch: 30, Current LR: 0.01, Loss: 0.094, Time: 3m13s\n",
      "Validation >>> Epoch: 30, Loss: 0.087, current lr: 0.01, F1 Score: 0.657, Time: 0m29s\n",
      "Training   >>> Epoch: 31, Current LR: 0.01, Loss: 0.094, Time: 3m10s\n",
      "Training   >>> Epoch: 32, Current LR: 0.01, Loss: 0.094, Time: 3m11s\n",
      "Validation >>> Epoch: 32, Loss: 0.086, current lr: 0.01, F1 Score: 0.662, Time: 0m30s\n",
      "Training   >>> Epoch: 33, Current LR: 0.01, Loss: 0.093, Time: 3m11s\n",
      "Training   >>> Epoch: 34, Current LR: 0.01, Loss: 0.092, Time: 3m11s\n",
      "Validation >>> Epoch: 34, Loss: 0.085, current lr: 0.01, F1 Score: 0.671, Time: 0m26s\n",
      "Training   >>> Epoch: 35, Current LR: 0.01, Loss: 0.092, Time: 3m12s\n",
      "Training   >>> Epoch: 36, Current LR: 0.01, Loss: 0.091, Time: 3m13s\n",
      "Validation >>> Epoch: 36, Loss: 0.085, current lr: 0.01, F1 Score: 0.677, Time: 0m29s\n",
      "Training   >>> Epoch: 37, Current LR: 0.01, Loss: 0.091, Time: 3m13s\n",
      "Training   >>> Epoch: 38, Current LR: 0.01, Loss: 0.090, Time: 3m13s\n",
      "Validation >>> Epoch: 38, Loss: 0.084, current lr: 0.01, F1 Score: 0.682, Time: 0m28s\n",
      "Training   >>> Epoch: 39, Current LR: 0.01, Loss: 0.090, Time: 3m12s\n",
      "Training   >>> Epoch: 40, Current LR: 0.01, Loss: 0.090, Time: 3m9s\n",
      "Validation >>> Epoch: 40, Loss: 0.083, current lr: 0.01, F1 Score: 0.685, Time: 0m28s\n",
      "Training   >>> Epoch: 41, Current LR: 0.01, Loss: 0.089, Time: 3m12s\n",
      "Training   >>> Epoch: 42, Current LR: 0.01, Loss: 0.089, Time: 3m13s\n",
      "Validation >>> Epoch: 42, Loss: 0.083, current lr: 0.01, F1 Score: 0.694, Time: 0m29s\n",
      "Training   >>> Epoch: 43, Current LR: 0.01, Loss: 0.088, Time: 3m12s\n",
      "Training   >>> Epoch: 44, Current LR: 0.01, Loss: 0.087, Time: 3m11s\n",
      "Validation >>> Epoch: 44, Loss: 0.083, current lr: 0.01, F1 Score: 0.700, Time: 0m30s\n",
      "Training   >>> Epoch: 45, Current LR: 0.01, Loss: 0.088, Time: 3m14s\n",
      "Training   >>> Epoch: 46, Current LR: 0.01, Loss: 0.087, Time: 3m13s\n",
      "Validation >>> Epoch: 46, Loss: 0.082, current lr: 0.01, F1 Score: 0.699, Time: 0m28s\n",
      "Training   >>> Epoch: 47, Current LR: 0.01, Loss: 0.087, Time: 3m13s\n",
      "Training   >>> Epoch: 48, Current LR: 0.01, Loss: 0.086, Time: 3m12s\n",
      "Validation >>> Epoch: 48, Loss: 0.082, current lr: 0.01, F1 Score: 0.702, Time: 0m29s\n",
      "Training   >>> Epoch: 49, Current LR: 0.01, Loss: 0.086, Time: 3m13s\n",
      "Training   >>> Epoch: 50, Current LR: 0.01, Loss: 0.086, Time: 3m12s\n",
      "Validation >>> Epoch: 50, Loss: 0.081, current lr: 0.01, F1 Score: 0.707, Time: 0m27s\n",
      "Training   >>> Epoch: 51, Current LR: 0.01, Loss: 0.085, Time: 3m13s\n",
      "Training   >>> Epoch: 52, Current LR: 0.01, Loss: 0.085, Time: 3m13s\n",
      "Validation >>> Epoch: 52, Loss: 0.081, current lr: 0.01, F1 Score: 0.709, Time: 0m28s\n",
      "Training   >>> Epoch: 53, Current LR: 0.01, Loss: 0.084, Time: 3m10s\n",
      "Training   >>> Epoch: 54, Current LR: 0.01, Loss: 0.084, Time: 3m13s\n",
      "Validation >>> Epoch: 54, Loss: 0.081, current lr: 0.01, F1 Score: 0.711, Time: 0m28s\n",
      "Training   >>> Epoch: 55, Current LR: 0.01, Loss: 0.083, Time: 3m11s\n",
      "Training   >>> Epoch: 56, Current LR: 0.01, Loss: 0.083, Time: 3m13s\n",
      "Validation >>> Epoch: 56, Loss: 0.081, current lr: 0.01, F1 Score: 0.715, Time: 0m29s\n",
      "Training   >>> Epoch: 57, Current LR: 0.01, Loss: 0.083, Time: 3m11s\n",
      "Training   >>> Epoch: 58, Current LR: 0.01, Loss: 0.083, Time: 3m11s\n",
      "Validation >>> Epoch: 58, Loss: 0.080, current lr: 0.01, F1 Score: 0.717, Time: 0m29s\n",
      "Training   >>> Epoch: 59, Current LR: 0.01, Loss: 0.082, Time: 3m13s\n",
      "Training   >>> Epoch: 60, Current LR: 0.01, Loss: 0.082, Time: 3m13s\n",
      "Validation >>> Epoch: 60, Loss: 0.080, current lr: 0.01, F1 Score: 0.719, Time: 0m29s\n",
      "Training   >>> Epoch: 61, Current LR: 0.01, Loss: 0.082, Time: 3m13s\n",
      "Training   >>> Epoch: 62, Current LR: 0.01, Loss: 0.081, Time: 3m13s\n",
      "Validation >>> Epoch: 62, Loss: 0.080, current lr: 0.01, F1 Score: 0.718, Time: 0m29s\n",
      "Training   >>> Epoch: 63, Current LR: 0.01, Loss: 0.081, Time: 3m13s\n",
      "Training   >>> Epoch: 64, Current LR: 0.01, Loss: 0.081, Time: 3m12s\n",
      "Validation >>> Epoch: 64, Loss: 0.079, current lr: 0.01, F1 Score: 0.727, Time: 0m27s\n",
      "Training   >>> Epoch: 65, Current LR: 0.01, Loss: 0.081, Time: 3m12s\n",
      "Training   >>> Epoch: 66, Current LR: 0.01, Loss: 0.080, Time: 3m10s\n",
      "Validation >>> Epoch: 66, Loss: 0.079, current lr: 0.01, F1 Score: 0.729, Time: 0m28s\n",
      "Training   >>> Epoch: 67, Current LR: 0.01, Loss: 0.080, Time: 3m14s\n",
      "Training   >>> Epoch: 68, Current LR: 0.01, Loss: 0.079, Time: 3m12s\n",
      "Validation >>> Epoch: 68, Loss: 0.079, current lr: 0.01, F1 Score: 0.727, Time: 0m29s\n",
      "Training   >>> Epoch: 69, Current LR: 0.01, Loss: 0.079, Time: 3m10s\n",
      "Training   >>> Epoch: 70, Current LR: 0.01, Loss: 0.079, Time: 3m12s\n",
      "Validation >>> Epoch: 70, Loss: 0.079, current lr: 0.01, F1 Score: 0.733, Time: 0m29s\n",
      "Training   >>> Epoch: 71, Current LR: 0.01, Loss: 0.078, Time: 3m12s\n",
      "Training   >>> Epoch: 72, Current LR: 0.01, Loss: 0.078, Time: 3m14s\n",
      "Validation >>> Epoch: 72, Loss: 0.079, current lr: 0.01, F1 Score: 0.732, Time: 0m30s\n",
      "Training   >>> Epoch: 73, Current LR: 0.01, Loss: 0.078, Time: 3m11s\n",
      "Training   >>> Epoch: 74, Current LR: 0.01, Loss: 0.078, Time: 3m11s\n",
      "Validation >>> Epoch: 74, Loss: 0.078, current lr: 0.01, F1 Score: 0.733, Time: 0m29s\n",
      "Training   >>> Epoch: 75, Current LR: 0.01, Loss: 0.077, Time: 3m11s\n",
      "Training   >>> Epoch: 76, Current LR: 0.01, Loss: 0.077, Time: 3m14s\n",
      "Validation >>> Epoch: 76, Loss: 0.078, current lr: 0.01, F1 Score: 0.733, Time: 0m28s\n",
      "Training   >>> Epoch: 77, Current LR: 0.01, Loss: 0.077, Time: 3m11s\n",
      "Training   >>> Epoch: 78, Current LR: 0.01, Loss: 0.077, Time: 3m12s\n",
      "Validation >>> Epoch: 78, Loss: 0.078, current lr: 0.01, F1 Score: 0.734, Time: 0m29s\n",
      "Training   >>> Epoch: 79, Current LR: 0.01, Loss: 0.076, Time: 3m12s\n",
      "Training   >>> Epoch: 80, Current LR: 0.01, Loss: 0.076, Time: 3m13s\n",
      "Validation >>> Epoch: 80, Loss: 0.078, current lr: 0.01, F1 Score: 0.736, Time: 0m30s\n",
      "Training concluded >>> Total Run Time: 276m58s\n",
      "Model size on disk: 54.99 MB\n"
     ]
    }
   ],
   "source": [
    "model_name = 'regnetMixPrec'\n",
    "\n",
    "epochs = 80\n",
    "optimizers = [['Adadelta', 0.01, True]]\n",
    "\n",
    "# Create DataLoaders for both train and validation sets\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "#run train and val\n",
    "for optimizer in optimizers:\n",
    "    optimizer_name = optimizer[0]\n",
    "    optimizer_starting_lr = optimizer[1]\n",
    "    optimizer_lr_scheduler_toggle = optimizer[2]\n",
    "    print(f'============================================+++++++=============================')\n",
    "    print(f\"Model: {model_name}, optimizer: {optimizer_name}, starting_lr= {optimizer_starting_lr}, lr_scheduler: {optimizer_lr_scheduler_toggle}\")\n",
    "    \n",
    "    #reset model\n",
    "    model = models.regnet_x_3_2gf(weights = models.RegNet_X_3_2GF_Weights.DEFAULT)\n",
    "    model.num_classes = 18\n",
    "    model.fc = torch.nn.Linear(model.fc.in_features, model.num_classes)  \n",
    "    \n",
    "    #call optimizer and starting learning rate:\n",
    "    if optimizer_name == 'Adadelta':\n",
    "        optimizer = torch.optim.Adadelta(model.parameters(), lr=optimizer_starting_lr)\n",
    "    elif optimizer_name == 'Adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=optimizer_starting_lr) \n",
    "    elif optimizer_name == 'SGD':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=optimizer_starting_lr)\n",
    "\n",
    "    \n",
    "    output_name = f'{model_name}_{optimizer_name}_{optimizer_starting_lr}'\n",
    "    train_val_run(model, optimizer, output_name, test_data, epochs, lr_scheduler_toggle = optimizer_lr_scheduler_toggle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
